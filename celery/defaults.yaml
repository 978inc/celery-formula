{% load_yaml as lookup_map %}
default:
  version: '4.0.2'
  source_hash: 'sha256=ca95d6f80ffd83817ff8368310b191b2198beeeb3bc6202c0ddf0ded566391cc'
  service: 'celeryd'

  # controls whether to download a relase from github or not. Leaving this False means we use pip
  from_source: False

  # should point to a directory which contains a "bin" directory where python and pip can be found
  # if you're using virutalenv, you would set this to whatever $VIRTUALENV_HOME resolves to, including
  # a named environment. (read: virtualenvwrapper.sh)
  bin_env: /usr
  
  prefix: /usr/local
  working_dir: /tmp/celery/working    # 
  config_dir: /etc/celery             # location of broker and worker configfiles
  run_dir: /var/run/celery            # location of pid file
  log_dir: /tmp/celery/log            # logging
  log_level: DEBUG
  broker: redis
  user: 'celery'
  # list of maps where each item can define options like concurrency and batch_size
  worker_queues: []
  # how many celery threads per machine
  total_workers: 4
  worker_timeout: 300
  
Debian: {}
RedHat:
  python_command: /usr/local/bin/python
  config_dir: /usr/local/etc/celery
  
{% endload %}

# Broker configuration
{% load_yaml as broker_defaults %}
url: 'redis://'
pool_limit: 10
connection:
  retry: true
  timeout: 60
  max_retries: 10
failover_strategy: ~
heartbeat: ~
login_method: ~
pool_limit: ~
use_ssl: ~
{% endload %}
## EOF Broker

{% load_yaml as worker_defaults %}
# Concurrency - change this to add more threads on each worker
concurrency: 1
# messages per thread to prefetch
prefetch_multiplier: 1
worker_lost_wait: ~
max_tasks_per_child: 1
task_time_limit: 3600
{% endload %}

## Celerybeat configuration options
{% load_yaml as beat_defaults %}
max_loop_interval: ~
schedule: ~
scheduler: ~
schedule_filename: ~
sync_every: ~
{% endload %}
## EOF Celerybeat

## Result backend configuration
{% load_yaml as result_defaults %}
compression: gzip
backend: filesystem
cache_max: ~
exchange: ~
exchange_type: ~
expires: ~
persistent: ~
serializer: json
{% endload %}
## EOF Result

## Task config options
{% load_yaml as task_defaults %}
# if true, ack happens after task is complete.
acks_late: false
# if true, will re-queue tasks even if acks_late is true
reject_on_worker_lost: false
# send tasks to the queue. If true, exec tasks locally
always_eager: false
# let worker exceptions bubble up
eager_propagates: true
# capture worker stack traces
remote_tracebacks: true
# track started
track_started: true

annotations: ~
compression: gzip
create_missing_queues: true
default_delivery_mode: persistent
default_queue: celery
default_rate_limit: ~
default_routing_key: ~

ignore_result: ~
publish_retry: ~
publish_retry_policy: ~
queues: ~
routes: ~
serializer: ~

{% endload %}

{% load_yaml as default_config %}
beat: {{ beat_defaults }}
broker: {{ broker_defaults }}
worker: {{ worker_defaults }}
task: {{ task_defaults }}
result: {{ result_defaults }}

# Any keys in this section will be treated as top level keys and will not be collapsed
enable_utc: true
timezone: UTC
accept_content: ['json']
{% endload %}
## EOF Celery Config
